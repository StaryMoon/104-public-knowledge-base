《数据科学导引》笔记
## 监督学习
- 模型：有监督学（拟合）（某种）函数
  - 如：假设空间小：参数模型（传统统计）
    - 基线性组合
      - 如傅里叶
      - 如"hat function"：每个$\phi_j$都是“三角波”，最后类似于“线性插值”
        - 特殊情况：直接线性插值，直接构造。假设空间为单元素集
  - 输出
    - 离散拓扑（猫0狗1数值无意义）：分类
    - 连续（数值有意义）：回归
- 假设空间之中怎么找？先有个“标准”
  - 经验误差最小（拟合）
    - 经验误差一般L2，也可以复杂一点（不是"parametric function"）
  - 正则项（关于参数$\theta$的）：“惩罚”复杂的模型
  - 两者关系：有参数控制。这个参数也是模型的参数，但不是学出来，而是手调。称为超参
- 算法：优化
- 例子
  - 学一元一维函数
  - 学imagenet分类
  - 学$\mathbb R^{3N}\to \mathbb R$的分子势能估计。利用平移旋转调换对称性
    - 先验怎么用？模型或loss处体现
  - 输出高维？image translation. image generation. 解pde（无穷维）
- 误差分解
  - 学不到最佳（空间太大）带来一部分estimation error. 比如你模型太复杂，有很多乱七八糟的函数。正则化减小这种效应
    - 思考：这个东西会不会饱和？
      - 不太可能，否则到了一定程度就越复杂越好。但也不是不可能。
      - 很多机器学习课本忽略了讨论这个
        - 特别是很多传统统计模型，很快就会approximation到0. 那就不用讨论这个
      - 学界称为double descent，有可能后面descent的更好。不是一个简单U形
    - 这个东西和overfitting, noise有区别！但是，没有noise也有estimation error
  - 学到最佳还是不够好（空间太小）带来一部分approximation error
## 无监督
不学函数。学分布（或：只学一些信息，“统计量”）